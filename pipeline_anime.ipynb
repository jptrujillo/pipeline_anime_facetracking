{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b3be7c7-51f4-47ea-b0ad-bb1190cb67e5",
   "metadata": {},
   "source": [
    "<b>ToDO:</b>\n",
    "- figure out how to recenter the predicted landmarks to actually match the image\n",
    "- adjust the manga_dataset class to not require a landmarks .txt file\n",
    "- adjust the loop in this script to not pull a landmarks file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "618527f6-1bb1-4ba6-b8cf-76490d26f3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(sys.path[0]))\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import csv\n",
    "from utils import *\n",
    "from transformers import BeitForImageClassification\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "from manga_dataset import MangaLandmarksDataset\n",
    "from transform import TransformsNoAugmentation\n",
    "from collections import OrderedDict\n",
    "from transformers import BeitForImageClassification\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c64d99a-1937-46b2-8089-342f6876ad2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocessing(pred, imgsz:int=24):\n",
    "    \"\"\"\n",
    "    ret `pred` as the parameters then\n",
    "    1. +0.5 for all preds-points and * imgsz(224) to get the exact position on the image\n",
    "    2. .view(-1, 60, 2) to get the x-y coordinates for all landmarks on manga faces\n",
    "    3. .squeeze() -> reducing array preds dimension\n",
    "    4. converting to the numpy()\n",
    "    \"\"\"\n",
    "    # print(pred.logits)\n",
    "    # pred = (pred.cpu() + 0.5) * 224\n",
    "    pred = (pred + 0.5) * imgsz\n",
    "    pred = pred.view(-1,60,2)\n",
    "    pred = pred.squeeze()\n",
    "    return pred.detach().numpy()\n",
    "\n",
    "def dataset_prepoessing(landmarks, img_size=224):\n",
    "    \"\"\"\n",
    "    dataset_preprocessing() -> \n",
    "        1. +0.5 for all landmarks, due to -0.5 when loading dataset\n",
    "        2. * imgs to get the exact position of that particular points on the image\n",
    "    \"\"\"\n",
    "    landmarks_noa = (landmarks + 0.5) * img_size\n",
    "    return landmarks_noa\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41466079-1498-4fa0-9d70-3319d23bbe53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65830768-c0ba-49d1-be0c-4482e441db8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Directory 'eval' already exists.\n",
      "Directory 'eval/beit_base_patch16_224_eval' already exists.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BeitForImageClassification(\n",
       "  (beit): BeitModel(\n",
       "    (embeddings): BeitEmbeddings(\n",
       "      (patch_embeddings): BeitPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): BeitEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (relative_position_bias): BeitRelativePositionBias()\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): Identity()\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (relative_position_bias): BeitRelativePositionBias()\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.00909090880304575)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (relative_position_bias): BeitRelativePositionBias()\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.0181818176060915)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (relative_position_bias): BeitRelativePositionBias()\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.027272727340459824)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (relative_position_bias): BeitRelativePositionBias()\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.036363635212183)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (relative_position_bias): BeitRelativePositionBias()\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.045454543083906174)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (6): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (relative_position_bias): BeitRelativePositionBias()\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.054545458406209946)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (7): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (relative_position_bias): BeitRelativePositionBias()\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.06363636255264282)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (8): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (relative_position_bias): BeitRelativePositionBias()\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.0727272778749466)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (9): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (relative_position_bias): BeitRelativePositionBias()\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.08181818574666977)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (10): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (relative_position_bias): BeitRelativePositionBias()\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.09090909361839294)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (11): BeitLayer(\n",
       "          (attention): BeitAttention(\n",
       "            (attention): BeitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (relative_position_bias): BeitRelativePositionBias()\n",
       "            )\n",
       "            (output): BeitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BeitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BeitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (drop_path): BeitDropPath(p=0.10000000149011612)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): Identity()\n",
       "    (pooler): BeitPooler(\n",
       "      (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=120, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_name = 'beit'\n",
    "weights_ls = ['microsoft/beit-base-patch16-224', \n",
    "    'microsoft/beit-base-patch16-384',\n",
    "    'microsoft/beit-large-patch16-224',\n",
    "    'microsoft/beit-large-patch16-384']\n",
    "selected_weight = 0\n",
    "variation = weights_ls[selected_weight].split('/')[-1]\n",
    "network = BeitForImageClassification.from_pretrained(weights_ls[selected_weight], ignore_mismatched_sizes=True)\n",
    "network.classifier = nn.Linear(in_features=network.classifier.in_features, out_features=120, bias=True)\n",
    "# LOAD WEIGHTS INTO MODELS\n",
    "# interested_epoch = 500\n",
    "# weights_path = f'weights/{model_name}_224_{interested_epoch}.pth'\n",
    "weights_path = 'weights/beit_rgb.pth'\n",
    "network.load_state_dict(torch.load(weights_path))\n",
    "network.to(device)\n",
    "print(f'Device: {device}')\n",
    "# print(f'Load weight@ {weights_path}')\n",
    "\n",
    "# CREATE DIRECTORY FOR EVALUATION\n",
    "create_directory_if_not_exists('eval')\n",
    "save_model_name = weights_ls[selected_weight].split(\"/\")[-1].replace(\"-\", \"_\")\n",
    "evaluation_name = f'{save_model_name}_eval'\n",
    "create_directory_if_not_exists(f'eval/{evaluation_name}')\n",
    "\n",
    "network.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dc35b4c-b54b-42d1-8d84-832cfb2dceb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = 'dataset'\n",
    "imgsz = 384\n",
    "test_dataset = MangaLandmarksDataset(TransformsNoAugmentation(), folder=dataset_folder, split_set='test', imgsz=imgsz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b8ce0d5-9275-47a5-b3ea-ab9caeaf429b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/dataset/FMA_test.jpg\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_folder = \"data/\" + dataset_folder\n",
    "\n",
    "files = os.listdir(data_folder)\n",
    "\n",
    "image_path = data_folder+ \"/\" + files[0]\n",
    "\n",
    "print(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "725a07c4-0de6-4044-9c57-1951f942b395",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = 'dataset'\n",
    "#imgsz = 384\n",
    "imgsz = 224\n",
    "test_dataset = MangaLandmarksDataset(TransformsNoAugmentation(), folder=dataset_folder, split_set='test', imgsz=imgsz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ddb599a-e11f-45ff-93f2-8187d3c075c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/dataset\\\\test\\\\FMA\\\\labels\\\\test\\\\FMA\\\\FMA_test.jpg']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.image_filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905acf55-da7c-44b2-8068-7ace8da381c7",
   "metadata": {},
   "source": [
    "This next block will be the actual pipeline. See what is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91a407c5-fbf5-4e40-8803-8f72771df006",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.27it/s]\n"
     ]
    }
   ],
   "source": [
    "class_length = dict()\n",
    "pred_ls = []\n",
    "gt_ls = []\n",
    "network.eval()\n",
    "\n",
    "# list of groundtruth landmarks coordinations\n",
    "#landmarks = []\n",
    "#for image, target, filename in test_dataset:\n",
    "#    landmarks.append(target)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for image, filename in tqdm(test_dataset):\n",
    "        image = image.to(device)\n",
    "        pred = postprocessing(network(image.view(1, 3, imgsz, imgsz)).logits.cpu(), imgsz=imgsz)\n",
    "        #landmark = dataset_prepoessing(landmarks=landmark, img_size=imgsz)\n",
    "        pred_ls.extend([pred]) # these seem to be the landmark coordinates, as predicted by the model\n",
    "        #gt_ls.extend([landmark]) # these are used for comparison to a ground-truth\n",
    "\n",
    "pred_class = dict()\n",
    "#gt_class = dict()\n",
    "class_length = dict()\n",
    "#gt_class['Overall'] = []\n",
    "pred_class['Overall'] = []\n",
    "class_length['Overall'] = 60\n",
    "for (i, name) in enumerate(FACIAL_LANDMARKS_IDXS.keys()):\n",
    "    #gt_class[name] = []\n",
    "    pred_class[name] = []\n",
    "#for gt in gt_ls:\n",
    "#    gt_class['Overall'].extend(gt)\n",
    "for pred in pred_ls:\n",
    "    pred_class['Overall'].extend(pred)\n",
    "\n",
    "\n",
    "\n",
    "for (i, name) in enumerate(FACIAL_LANDMARKS_IDXS.keys()):\n",
    "    (j, k) = FACIAL_LANDMARKS_IDXS[name]\n",
    "    class_length[name] = k-j\n",
    "    for gt_each in gt_ls:\n",
    "        gt_class[name].extend(gt_each[j:k])\n",
    "    for pred_each in pred_ls:\n",
    "        pred_class[name].extend(pred_each[j:k])\n",
    "\n",
    "chin_normalized_ls = []\n",
    " \n",
    "for i, coord in enumerate(gt_ls):\n",
    "    chin_normalized_ls.append(euclidean_distance(coord[:17][0], coord[:17][-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d4679d5-45dc-45e5-8870-b3a7035face8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class TransformsNoAugmentation in module transform:\n",
      "\n",
      "class TransformsNoAugmentation(builtins.object)\n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__(self, image)\n",
      " |      Call self as a function.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(TransformsNoAugmentation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d2e484e-7477-4be2-b192-41c3d0b9af63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/dataset\\\\test\\\\FMA\\\\labels\\\\test\\\\FMA\\\\FMA_test.jpg'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e03ab40-311c-49ad-96e8-b53c9eb25dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgsz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbef0aa0-8ed2-4ec4-aac9-b07e0b99a08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_copy = cv2.imread(image_path)\n",
    "image_copy = cv2.resize(image_copy, (384, 384))\n",
    "shape = pred_ls\n",
    "shape = np.array([[round(val+50) for val in ls] for ls in shape[0]])\n",
    "output = visualize_facial_landmarks(image_copy,shape)\n",
    "\n",
    "#cv2.imshow('image',output)\n",
    "#cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2fe645f-048f-4622-bf14-5a154f94b84f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shape = pred_ls\n",
    " \n",
    "#shape2 = [[int(val) for val in ls] for ls in shape[0]]\n",
    "\n",
    "# create two copies of the input image -- one for the\n",
    "# overlay and one for the final output image\n",
    "colors=None\n",
    "alpha=0.75\n",
    "\n",
    "\n",
    "overlay = image_copy.copy()\n",
    "output = image_copy.copy()\n",
    "# if the colors list is None, initialize it with a unique\n",
    "# color for each facial landmark region\n",
    "if colors is None:\n",
    "    colors = [(19, 199, 109), (79, 76, 240), (230, 159, 23),\n",
    "              (168, 100, 168), (158, 163, 32), (100, 60, 200), \n",
    "              (100, 60, 200), (100, 60, 200), (180, 42, 220)]\n",
    "    \n",
    "# loop over the facial landmark regions individually\n",
    "for (i, name) in enumerate(FACIAL_LANDMARKS_IDXS.keys()):\n",
    "    # grab the (x, y)-coordinates associated with the\n",
    "    # face landmark\n",
    "    (j, k) = FACIAL_LANDMARKS_IDXS[name]\n",
    "    pts = shape[j:k]\n",
    "   # pts = pts*1.5\n",
    "    # check if are supposed to draw the jawline\n",
    "    if name == \"ChinContour\":\n",
    "        # since the jawline is a non-enclosed facial region,\n",
    "        # just draw lines between the (x, y)-coordinates\n",
    "        for l in range(1, len(pts)):\n",
    "            ptA = tuple(pts[l - 1])\n",
    "            ptB = tuple(pts[l])\n",
    "            cv2.line(overlay, ptA, ptB, colors[i], 2)\n",
    "    # otherwise, compute the convex hull of the facial\n",
    "    # landmark coordinates points and display it\n",
    "    else:\n",
    "        hull = cv2.convexHull(pts)\n",
    "        cv2.drawContours(overlay, [hull], -1, colors[i], -1)\n",
    "\n",
    "# apply the transparent overlay\n",
    "cv2.addWeighted(overlay, alpha, output, 1 - alpha, 0, output)\n",
    "\n",
    "cv2.imshow('image',output)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "983d4dd2-3a47-4c33-9b92-9e853e128592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[163, 215],\n",
       "       [164, 210],\n",
       "       [169, 207],\n",
       "       [182, 209],\n",
       "       [184, 210],\n",
       "       [179, 216],\n",
       "       [179, 217],\n",
       "       [174, 225],\n",
       "       [167, 224],\n",
       "       [161, 219]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0ea7d27-2732-4fde-b6b6-46b6a37578cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[86, 139],\n",
       " [92, 160],\n",
       " [109, 179],\n",
       " [109, 197],\n",
       " [121, 213],\n",
       " [133, 226],\n",
       " [149, 240],\n",
       " [161, 253],\n",
       " [177, 262],\n",
       " [196, 260],\n",
       " [210, 250],\n",
       " [224, 236],\n",
       " [235, 216],\n",
       " [245, 197],\n",
       " [257, 178],\n",
       " [268, 157],\n",
       " [270, 133],\n",
       " [72, 85],\n",
       " [83, 89],\n",
       " [96, 89],\n",
       " [116, 101],\n",
       " [130, 101],\n",
       " [168, 97],\n",
       " [193, 93],\n",
       " [219, 89],\n",
       " [242, 82],\n",
       " [263, 81],\n",
       " [92, 142],\n",
       " [99, 138],\n",
       " [111, 138],\n",
       " [123, 141],\n",
       " [132, 141],\n",
       " [136, 150],\n",
       " [121, 149],\n",
       " [112, 152],\n",
       " [101, 148],\n",
       " [93, 144],\n",
       " [198, 143],\n",
       " [214, 138],\n",
       " [229, 136],\n",
       " [243, 134],\n",
       " [253, 134],\n",
       " [251, 140],\n",
       " [242, 143],\n",
       " [230, 141],\n",
       " [213, 140],\n",
       " [195, 146],\n",
       " [120, 141],\n",
       " [223, 136],\n",
       " [157, 178],\n",
       " [163, 215],\n",
       " [164, 210],\n",
       " [169, 207],\n",
       " [182, 209],\n",
       " [184, 210],\n",
       " [179, 216],\n",
       " [179, 217],\n",
       " [174, 225],\n",
       " [167, 224],\n",
       " [161, 219]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape2 = [[round(val) for val in ls] for ls in shape]\n",
    "shape2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "caef58f5-0832-42d8-990f-98466a7b32e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part just writes thedata to a file. Currently, one file per image/frame. \n",
    "# class_length = dict()\n",
    "data = [\n",
    "    [\"Overall\"],\n",
    "    [\"ChinContour\"],\n",
    "    [\"RightEyeBrow\"],\n",
    "    [\"LeftEyeBrow\"],\n",
    "    [\"RightEye\"],\n",
    "    [\"LeftEye\"],\n",
    "    [\"RightPupil\"],\n",
    "    [\"LeftPupil\"],\n",
    "    [\"Nose\"],\n",
    "    [\"Mouth\"],\n",
    "    ]\n",
    "\n",
    "# needs to be adjusted when this is all put into a loop\n",
    "filepath_short = files[0].split(\".\")[0]\n",
    "\n",
    "csv_filename = os.path.join(\"./tracking_output/\", f'{filepath_short}_xy.csv')\n",
    "\n",
    "data = np.array(data)\n",
    "\n",
    "# save image with overlay\n",
    "cv2.imwrite(os.path.join(\"./tracking_output/\", f'{filepath_short}_tracked.jpg'), output)\n",
    "\n",
    "# save coordinate data\n",
    "with open(csv_filename, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    #writer.writerows(data[:,1:])\n",
    "    writer.writerows([[\"x\", \"y\"]])\n",
    "    writer.writerows(shape2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
